{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tf_f3jXw961c"
   },
   "source": [
    "# Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b4e5FJj29-ak"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "MODEL_TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "GA_SCORES_PATH  = 'GA_Scores/'\n",
    "REPORTS_PATH = 'Reports/'\n",
    "HYPERPARAMS_PATH = 'Hyperparams/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_json(feature_vector, root_path, file_name):\n",
    "    with open(root_path + file_name, 'w') as outfile:\n",
    "        json.dump(feature_vector, outfile)\n",
    "\n",
    "def load_json(root_path, file_name):\n",
    "    with open(root_path + file_name) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dDSVnTE3QGf"
   },
   "source": [
    "# 1 - Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DkpgKxMv3OkI"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41231/3922930893.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imutils import paths\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Librerías necesarias para aplicar LBP\n",
    "from skimage.transform import rotate\n",
    "from skimage.feature import hog, local_binary_pattern, multiblock_lbp\n",
    "from skimage import data\n",
    "from skimage.color import label2rgb\n",
    "from skimage.transform import integral_image\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report,confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLgGPCuwr4vP"
   },
   "source": [
    "# 2 - Variables Goblales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NLGhgjdsKsv"
   },
   "outputs": [],
   "source": [
    "ROOT_PATH  = './data/INRIAPerson'\n",
    "TRAIN_PATH = '/Train/'\n",
    "\n",
    "ext = ['png', 'jpg', 'gif']    # Add image formats here\n",
    "\n",
    "imagePaths = list(paths.list_images(ROOT_PATH + TRAIN_PATH))\n",
    "\n",
    "eps=1e-7\n",
    "numPoints = 24\n",
    "radius = 8\n",
    "\n",
    "height = 128\n",
    "weight = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWMCAPSXsOQ8"
   },
   "source": [
    "# 3 - Cargamos las Imagenes y Generamos los Embeddings de HOG y LBP\n",
    "\n",
    "En este apartado cargamos las imagenes, aplicamos los sliding windows para recorrer cada imagen bloque por bloque y calcular sus histogramas, para cada bloque aplicamos **Histogram of Oriented Gradient** (HOG) como clasificador de caracteristicas y **Local Binary Pattern** (LBP) para clasificación de texturas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMFEp-sI2gaX",
    "outputId": "d56efdbc-a0b5-457e-fca7-41a7754091ef",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Extracted_Names = []\n",
    "Extracted_Embeddings = []\n",
    "\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "        if(int(i) >= 10000 and int(i) <= 15000):\n",
    "          continue\n",
    "        print(\"[INFO] processing image {}/{}\".format(i + 1,len(imagePaths)))\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "        img = cv2.imread(imagePath)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        resized = resize(gray, (128, 64))\n",
    "        for i in range(16, height, 16):\n",
    "          # print(\"i: \" + str(i))\n",
    "          for j in range(16, weight, 16):\n",
    "              # print(\"j: \" + str(j))\n",
    "              box = resized[i-16:i,j-16:j]\n",
    "              lbp = local_binary_pattern(box, numPoints, radius, method=\"uniform\")\n",
    "              (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, numPoints + 3), range=(0, numPoints + 2))\n",
    "              # normalize the histogram\n",
    "              hist = hist.astype(\"float\")\n",
    "              hist /= (hist.sum() + eps)\n",
    "\n",
    "              lbp_embedding = hist\n",
    "              # print(\"LBP\",lbp_embedding.shape)\n",
    "              hog_embedding = hog(box, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False, multichannel=False)\n",
    "              # print(\"HOG\",hog_embedding.shape)\t\n",
    "              embedding = np.append(hog_embedding.ravel(),lbp_embedding.ravel())\n",
    "              # print(\"TOTAL\",embedding.shape)\t\n",
    "              Extracted_Names.append(name)\n",
    "              Extracted_Embeddings.append(embedding)\n",
    "\n",
    "Extracted_Embeddings = np.array(Extracted_Embeddings)\n",
    "print(Extracted_Embeddings.shape)\n",
    "# dump the HOG and LBP embeddings + names to disk\n",
    "print(\"[INFO] serializing encodings...\")\n",
    "data = {\"embeddings\": Extracted_Embeddings, \"names\": Extracted_Names}\n",
    "f = open(\"embeddings.pickle\", \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UCQazPXug7g"
   },
   "source": [
    "# 4 - Entrenamos el modelo con los Embeddings\n",
    "\n",
    "Cargaremos los embeddings que guardamos en el disco, entrenaremos el modelo con XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDgzrBGq_567",
    "outputId": "6345dd93-58c2-420f-90da-5ad9808868aa"
   },
   "outputs": [],
   "source": [
    "print(type(Extracted_Embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CwT5LQycrz6x",
    "outputId": "02be5089-5cc7-4067-a327-8ca2359042f0"
   },
   "outputs": [],
   "source": [
    "# load the embeddings\n",
    "print(\"[INFO] loading pictures embeddings...\")\n",
    "data = pickle.loads(open(\"embeddings.pickle\", \"rb\").read())\n",
    "\n",
    "# encode the labels\n",
    "print(\"[INFO] encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(data[\"names\"])\n",
    "print(labels)\n",
    "# dividing in train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"embeddings\"], labels, test_size=0.20)\n",
    "\n",
    "# # train the model used to accept the 3806-d embeddings of the pedestrian and\n",
    "# # then produce the actual pedestrain recognition\n",
    "# print(\"[INFO] training model...\")\n",
    "\n",
    "# xgboost = XGBClassifier()\n",
    "# xgboost.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# print(\"Accuracy on training set: {:.3f}\".format(xgboost.score(X_train, y_train)))\n",
    "# print(\"Accuracy on validation set: {:.3f}\".format(xgboost.score(X_test, y_test)))\n",
    "\n",
    "# # write the actual face recognition model to disk\n",
    "# f = open(\"classifier.pickle\", \"wb\")\n",
    "# f.write(pickle.dumps(xgboost))\n",
    "# f.close()\n",
    "\n",
    "# # write the label encoder to disk\n",
    "# f = open(\"le.pickle\", \"wb\")\n",
    "# f.write(pickle.dumps(le))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1_y7_khRkmx",
    "outputId": "96391514-c6c3-418b-8322-55545d97a4f5"
   },
   "outputs": [],
   "source": [
    "# xgb_predict = xgboost.predict(X_test)\n",
    "# xgb_predict\n",
    "\n",
    "# print(confusion_matrix(y_test,xgb_predict))\n",
    "# print(classification_report(y_test,xgb_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "ZyJ-3s4UuKO9",
    "outputId": "9ca1dd34-a473-44e0-e14e-be7ab89e2125"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Extracted_Names = []\n",
    "Extracted_Embeddings = []\n",
    "\n",
    "eps=1e-7\n",
    "numPoints = 24\n",
    "radius = 9\n",
    "\n",
    "height = 128\n",
    "weight = 64\n",
    "\n",
    "#for (k, image) in enumerate(resized_images[0]):\n",
    "image = resized_images[0]\n",
    "\n",
    "for i in range(16, height, 16):\n",
    "\tprint(\"i: \" + str(i))\n",
    "\tfor j in range(16, weight, 16):\n",
    "\t\t\tprint(\"j: \" + str(j))\n",
    "\t\t\tbox = image[i-16:i,j-16:j]\n",
    "\t\t\tlbp = local_binary_pattern(box, numPoints, radius, method=\"uniform\")\n",
    "\t\t\t(hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, numPoints + 3), range=(0, numPoints + 2))\n",
    "\t\t\t# normalize the histogram\n",
    "\t\t\thist = hist.astype(\"float\")\n",
    "\t\t\thist /= (hist.sum() + eps)\n",
    "\n",
    "\t\t\tlbp_embedding = hist\n",
    "\t\t\tprint(\"LBP\",lbp_embedding.shape)\n",
    "\t\t\thog_embedding = hog(box, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False, multichannel=False)\n",
    "\t\t\tprint(\"HOG\",hog_embedding.shape)\t\n",
    "\t\t\tembedding = np.append(hog_embedding.ravel(),lbp_embedding.ravel())\n",
    "\t\t\tprint(\"TOTAL\",embedding.shape)\t\n",
    "\t\t\t#Extracted_Names.append(name)\n",
    "\t\t\tExtracted_Embeddings.append(embedding)\n",
    "\t \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#embedding = pca.transform(embedding.reshape(1, -1))  \n",
    "#prediction = recognizer.predict(embedding.reshape(1, -1))\n",
    "#cv2.rectangle(nroi,(j,i),(j-121,i-121),(255,0,0),2)\n",
    "#if(prediction == 1):\n",
    "#\tcv2.rectangle(col,(j,i),(j-39,i-39),(0,0,255),1)\n",
    "#else:\n",
    "#\tcv2.rectangle(col,(j,i),(j-39,i-39),(0,255,0),1)\n",
    "#jc+=1\n",
    "#cv2.imshow('temp2',col)\n",
    "#cv2.waitKey(1)\n",
    "\t\t \n",
    "#################################################################################\n",
    "\n",
    "\tprint(\"[INFO] processing image {}/{}\".format(i + 1,len(image)))\n",
    "\t#name = image.split(os.path.sep)[-2]\n",
    "\t# compute the Local Binary Pattern representation\n",
    "\t# of the image, and then use the LBP representation\n",
    "\t# to build the histogram of patterns\\\n",
    "\thist = local_binary_pattern(image, numPoints, radius, method=\"uniform\")\n",
    "\t(hist, _) = np.histogram(hist.ravel(), bins=np.arange(0, numPoints + 3), range=(0, numPoints + 2))\n",
    "\t#normalize the histogram\n",
    "\thist = hist.astype(\"float\")\n",
    "\thist /= (hist.sum() + eps)\n",
    "  \n",
    "\tlbp_embedding = hist\n",
    "\tprint(\"LBP\",lbp_embedding.shape)\n",
    "\thog_embedding = hog(image, orientations=9, pixels_per_cell=(16, 16), cells_per_block=(2, 2), visualize=False, multichannel=False)\n",
    "\tprint(\"HOG\",hog_embedding.shape)\t\n",
    "\tembedding = np.append(hog_embedding.ravel(),lbp_embedding.ravel())\n",
    "\tprint(\"TOTAL\",embedding.shape)\t\n",
    "\t#Extracted_Names.append(name)\n",
    "\tExtracted_Embeddings.append(embedding)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UTlDVr103jRZ",
    "outputId": "308fb5a1-5666-4d89-b23e-93f67c5a1e05"
   },
   "outputs": [],
   "source": [
    "print(len(Extracted_Embeddings)*62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "CpuL-9OE5mcY",
    "outputId": "016768ee-c03b-48b8-bd28-e8cda711b7a2"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "METHOD = 'uniform'\n",
    "plt.rcParams['font.size'] = 16\n",
    "\n",
    "\n",
    "def plot_circle(ax, center, radius, color):\n",
    "    circle = plt.Circle(center, radius, facecolor=color, edgecolor='0.5')\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "\n",
    "def plot_lbp_model(ax, binary_values):\n",
    "    \"\"\"Draw the schematic for a local binary pattern.\"\"\"\n",
    "    # Geometry spec\n",
    "    theta = np.deg2rad(45)\n",
    "    R = 1\n",
    "    r = 0.15\n",
    "    w = 1.5\n",
    "    gray = '0.5'\n",
    "\n",
    "    # Draw the central pixel.\n",
    "    plot_circle(ax, (0, 0), radius=r, color=gray)\n",
    "    # Draw the surrounding pixels.\n",
    "    for i, facecolor in enumerate(binary_values):\n",
    "        x = R * np.cos(i * theta)\n",
    "        y = R * np.sin(i * theta)\n",
    "        plot_circle(ax, (x, y), radius=r, color=str(facecolor))\n",
    "\n",
    "    # Draw the pixel grid.\n",
    "    for x in np.linspace(-w, w, 4):\n",
    "        ax.axvline(x, color=gray)\n",
    "        ax.axhline(x, color=gray)\n",
    "\n",
    "    # Tweak the layout.\n",
    "    ax.axis('image')\n",
    "    ax.axis('off')\n",
    "    size = w + 0.2\n",
    "    ax.set_xlim(-size, size)\n",
    "    ax.set_ylim(-size, size)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(ncols=5, figsize=(7, 2))\n",
    "\n",
    "titles = ['flat', 'flat', 'edge', 'corner', 'non-uniform']\n",
    "\n",
    "binary_patterns = [np.zeros(16),\n",
    "                   np.ones(16),\n",
    "                   np.hstack([np.ones(12), np.zeros(12)]),\n",
    "                   np.hstack([np.zeros(12), np.ones(12)]),\n",
    "                   [1, 0, 0, 1, 1, 1, 0, 0]]\n",
    "\n",
    "for ax, values, name in zip(axes, binary_patterns, titles):\n",
    "    plot_lbp_model(ax, values)\n",
    "    ax.set_title(name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "8W_GbZA65VfV",
    "outputId": "db7efbb9-a3da-46b9-ffbe-a6c5baf1f9ae"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# settings for LBP\n",
    "radius = 3\n",
    "n_points = 8 * radius\n",
    "\n",
    "\n",
    "def overlay_labels(image, lbp, labels):\n",
    "    mask = np.logical_or.reduce([lbp == each for each in labels])\n",
    "    return label2rgb(mask, image=image, bg_label=0, alpha=0.5)\n",
    "\n",
    "\n",
    "def highlight_bars(bars, indexes):\n",
    "    for i in indexes:\n",
    "        bars[i].set_facecolor('r')\n",
    "\n",
    "\n",
    "#image = cv2.cvtColor(resized_images[256], cv2.COLOR_BGR2GRAY)\n",
    "lbp = local_binary_pattern(gray, n_points, radius, METHOD)\n",
    "\n",
    "\n",
    "def hist(ax, lbp):\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    return ax.hist(lbp.ravel(), density=True, bins=n_bins, range=(0, n_bins),\n",
    "                   facecolor='0.5')\n",
    "\n",
    "\n",
    "# plot histograms of LBP of textures\n",
    "fig, (ax_img, ax_hist) = plt.subplots(nrows=2, ncols=3, figsize=(9, 6))\n",
    "plt.gray()\n",
    "\n",
    "titles = ('edge', 'flat', 'corner')\n",
    "w = width = radius - 1\n",
    "edge_labels = range(n_points // 2 - w, n_points // 2 + w + 1)\n",
    "flat_labels = list(range(0, w + 1)) + list(range(n_points - w, n_points + 2))\n",
    "i_14 = n_points // 4            # 1/4th of the histogram\n",
    "i_34 = 3 * (n_points // 4)      # 3/4th of the histogram\n",
    "corner_labels = (list(range(i_14 - w, i_14 + w + 1)) +\n",
    "                 list(range(i_34 - w, i_34 + w + 1)))\n",
    "\n",
    "label_sets = (edge_labels, flat_labels, corner_labels)\n",
    "\n",
    "for ax, labels in zip(ax_img, label_sets):\n",
    "    ax.imshow(overlay_labels(gray, lbp, labels))\n",
    "\n",
    "for ax, labels, name in zip(ax_hist, label_sets, titles):\n",
    "    counts, _, bars = hist(ax, lbp)\n",
    "    highlight_bars(bars, labels)\n",
    "    ax.set_ylim(top=np.max(counts[:-1]))\n",
    "    ax.set_xlim(right=n_points + 2)\n",
    "    ax.set_title(name)\n",
    "\n",
    "ax_hist[0].set_ylabel('Percentage')\n",
    "for ax in ax_img:\n",
    "    ax.axis('off')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jI5lrWtj_EN5"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDiQq8BIlVEt"
   },
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzlZkA18lklW"
   },
   "source": [
    "## Initialize Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eJM-G-hlYiG"
   },
   "outputs": [],
   "source": [
    "def initilialize_poplulation(numberOfParents):\n",
    "\n",
    "    maxDepth   = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
    "    regAlpha   = np.empty([numberOfParents, 1])\n",
    "    regLambda  = np.empty([numberOfParents, 1])\n",
    "    gammaValue = np.empty([numberOfParents, 1])\n",
    "    subSample  = np.empty([numberOfParents, 1])\n",
    "    learningRate = np.empty([numberOfParents, 1])\n",
    "    nEstimators  = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
    "    minChildWeight  = np.empty([numberOfParents, 1])\n",
    "    colSampleByTree =  np.empty([numberOfParents, 1])\n",
    "\n",
    "    # space={'max_depth': hp.quniform(\"max_depth\", 3, 25, 1),\n",
    "#         'gamma': hp.uniform ('gamma', 1,9),\n",
    "#         'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "#         'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "#         'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "#         'min_child_weight' : hp.quniform('min_child_weight', 0, 15, 1),\n",
    "#         'n_estimators': hp.quniform('n_estimators', 100, 4000, 100),\n",
    "#         'tree_method': 'gpu_hist'\n",
    "#     }\n",
    "    for i in range(numberOfParents):\n",
    "        # learningRate[i] = round(random.uniform(0.01, 1), 2)\n",
    "        # nEstimators[i] = random.randrange(100, 2000, step = 150)\n",
    "        # maxDepth[i] = int(random.randrange(1, 20, step= 1))\n",
    "        # minChildWeight[i] = round(random.uniform(0.01, 15.0), 1)\n",
    "        # gammaValue[i] = round(random.uniform(0.01, 10.0), 2)\n",
    "        # subSample[i] = round(random.uniform(0.01, 1.0), 2)\n",
    "        # colSampleByTree[i] = round(random.uniform(0.01, 1.0), 2)\n",
    "        # regAlpha[i]  = round(random.uniform(40,180), 1)\n",
    "        # regLambda[i] = round(random.uniform(0,1), 3)\n",
    "        \n",
    "        learningRate[i] = round(random.uniform(0.01, 1), 2)\n",
    "        maxDepth[i] = int(random.randrange(1, 20, step= 1))\n",
    "        minChildWeight[i] = round(random.uniform(0.01, 15.0), 1)\n",
    "\n",
    "    \n",
    "    # population = np.concatenate((learningRate, nEstimators, maxDepth, minChildWeight, gammaValue, subSample, colSampleByTree, regAlpha, regLambda), axis= 1)\n",
    "    population = np.concatenate((learningRate, maxDepth, minChildWeight), axis= 1)\n",
    "  \n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZvASchWlouN"
   },
   "source": [
    "## Fitness Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejrLUgtsloI1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def fitness_f1score(y_true, y_pred):\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=1)\n",
    "    \n",
    "    fitness = round(auc(fpr, tpr), 4)\n",
    "\n",
    "    return fitness # Train the data annd find fitness score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XfWg497ltEW"
   },
   "source": [
    "## Evaluate Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbTYSO88lq8m"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_population(population, dMatrixTrain, dMatrixTest, y_test):\n",
    "    fScore = []\n",
    "    for i in range(population.shape[0]):\n",
    "        # param = { 'objective':'multi:softprob',\n",
    "        #           'tree_method': 'gpu_hist',\n",
    "        #           'num_class': 3,\n",
    "        #           'learning_rate': population[i][0],\n",
    "        #           'n_estimators': population[i][1], \n",
    "        #           'max_depth': int(population[i][2]), \n",
    "        #           'min_child_weight': population[i][3],\n",
    "        #           'gamma': population[i][4], \n",
    "        #           'subsample': population[i][5],\n",
    "        #           'colsample_bytree': population[i][6],\n",
    "        #           'reg_alpha': population[i][7],\n",
    "        #           'reg_lambda': population[i][8]\n",
    "        #         }\n",
    "\n",
    "        param = { 'objective':'multi:softprob',\n",
    "                  'tree_method': 'gpu_hist',\n",
    "                  'num_class': 3,\n",
    "                  'learning_rate': population[i][0],\n",
    "                  'max_depth': int(population[i][1]), \n",
    "                  'min_child_weight': population[i][2]\n",
    "                }\n",
    "\n",
    "        num_round = 100\n",
    "        xgb.set_config(verbosity=0)\n",
    "        xgbT = xgb.train(param,\n",
    "                         dMatrixTrain,\n",
    "                         num_round)\n",
    "\n",
    "        preds = xgbT.predict(dMatrixTest)\n",
    "        \n",
    "        single_predictions = [np.argmax(pred) for pred in preds]\n",
    "        # preds = preds > 0.5\n",
    "\n",
    "        fScore.append(fitness_f1score(y_test, single_predictions))\n",
    "\n",
    "    return fScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VSNcm72lwmn"
   },
   "source": [
    "## Parents Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pll0gi4dlwJ2"
   },
   "outputs": [],
   "source": [
    "# Select parents for mating\n",
    "def new_parents_selection(population, fitness, numParents):\n",
    "    selectedParents = np.empty((numParents, population.shape[1])) # Create an array to store fittest parents.\n",
    "\n",
    "\n",
    "    current_population = population\n",
    "    current_fitness = fitness\n",
    "\n",
    "    current_selected_parents_number = 0\n",
    "\n",
    "    while current_selected_parents_number < numParents:\n",
    "        print(f'Number of parents selected: {current_selected_parents_number}')\n",
    "        population_fitness = np.sum(current_fitness)\n",
    "\n",
    "        individuals_probability_to_be_selected = current_fitness/population_fitness\n",
    "\n",
    "        \n",
    "        random_number = random.uniform(0,1)\n",
    "\n",
    "        for parentId in range(numParents): \n",
    "            if sum(individuals_probability_to_be_selected[:parentId]) > random_number:\n",
    "                selectedParents[parentId, :] = current_population[parentId,:]\n",
    "                current_population = np.delete(current_population, parentId,0)\n",
    "                current_fitness = np.delete(current_fitness, parentId,0)\n",
    "                current_selected_parents_number += 1\n",
    "                break\n",
    "            \n",
    "\n",
    "        # for parentId in range(numParents):\n",
    "        #     bestFitnessId = np.where(fitness == np.max(fitness))\n",
    "        #     bestFitnessId  = bestFitnessId[0][0]\n",
    "        #     selectedParents[parentId, :] = population[bestFitnessId, :]\n",
    "        #     fitness[bestFitnessId] = -1 # Set this value to negative, in case of F1-score, so this parent is not selected again\n",
    "\n",
    "\n",
    "    return selectedParents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kzi-viIl0aH"
   },
   "source": [
    "## Population Crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vsa7S-Kblzo-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Mate these parents to create children having parameters from these parents (we are using uniform crossover method)\n",
    "'''\n",
    "def crossover_uniform(parents, childrenSize):\n",
    "    \n",
    "    crossoverPointIndex  = np.arange(0, np.uint8(childrenSize[1]), 1, dtype= np.uint8) # get all the index\n",
    "    crossoverPointIndex1 = np.random.randint(0, np.uint8(childrenSize[1]), np.uint8(childrenSize[1]/2)) # select half  of the indexes randomly\n",
    "    crossoverPointIndex2 = np.array(list(set(crossoverPointIndex) - set(crossoverPointIndex1))) #select leftover indexes\n",
    "    \n",
    "    children = np.empty(childrenSize)\n",
    "    \n",
    "    '''\n",
    "    Create child by choosing parameters from two parents selected using new_parent_selection function. The parameter values\n",
    "    will be picked from the indexes, which were randomly selected above. \n",
    "    '''\n",
    "    for i in range(childrenSize[0]):\n",
    "        \n",
    "        #find parent 1 index \n",
    "        parent1_index = i%parents.shape[0]\n",
    "        #find parent 2 index\n",
    "        parent2_index = (i+1)%parents.shape[0]\n",
    "        #insert parameters based on random selected indexes in parent 1\n",
    "        children[i, crossoverPointIndex1] = parents[parent1_index, crossoverPointIndex1]\n",
    "        #insert parameters based on random selected indexes in parent 1\n",
    "        children[i, crossoverPointIndex2] = parents[parent2_index, crossoverPointIndex2]\n",
    "\n",
    "    return children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOtLg8Vyl6Hy"
   },
   "source": [
    "## Childs Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qvOF5D4l2fe"
   },
   "outputs": [],
   "source": [
    "def mutation(crossover, numberOfParameters):\n",
    "    # Define minimum and maximum values allowed for each parameterminMaxValue = np.zeros((numberOfParameters, 2))\n",
    "    minMaxValue = np.zeros((numberOfParameters, 2))\n",
    "\n",
    "    minMaxValue[0:]  = [0.01, 1.0]  # min/max learning rate\n",
    "    minMaxValue[1,:] = [1, 15]      # min/max depth\n",
    "    minMaxValue[2,:] = [0, 10.0]    # min/max child_weight\n",
    " \n",
    "    # Mutation changes a single gene in each offspring randomly.\n",
    "    mutationValue = 0\n",
    "    parameterSelect = np.random.randint(0, numberOfParameters, 1)\n",
    "\n",
    "    print(parameterSelect)\n",
    "\n",
    "    if parameterSelect == 0: # learning_rate\n",
    "        # mutationValue = round(np.random.uniform(-0.2, 0.2), 2)\n",
    "        mutationValue = round(random.uniform(-0.1, 0.1), 3)\n",
    "    if parameterSelect == 1: # max_depth\n",
    "        # mutationValue = np.random.randint(-3, 3, 1)\n",
    "        mutationValue = int(random.randrange(-3, 3, step= 1))\n",
    "    if parameterSelect == 2: # min_child_weight\n",
    "        # mutationValue = round(np.random.uniform(5, 5), 2)\n",
    "        mutationValue = round(random.uniform(-5, 5), 1)\n",
    "\n",
    "\n",
    "    # Introduce mutation by changing one parameter, and set to max or min if it goes out of range\n",
    "    for idx in range(crossover.shape[0]):\n",
    "        crossover[idx, parameterSelect] = crossover[idx, parameterSelect] + mutationValue\n",
    "\n",
    "        if(crossover[idx, parameterSelect] > minMaxValue[parameterSelect, 1]):\n",
    "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 1]\n",
    "\n",
    "        if(crossover[idx, parameterSelect] < minMaxValue[parameterSelect, 0]):\n",
    "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 0]\n",
    "\n",
    "    return crossover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBhaP3nNmPFN"
   },
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f9ckwZZHmNDb",
    "outputId": "188354c5-ad86-46f3-fde8-1dbcbc5f6d22"
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test  = sc.transform(X_test)\n",
    "\n",
    "# XGboost Classifier\n",
    "# model xgboost\n",
    "# use xgboost API now\n",
    "\n",
    "import xgboost as xgb\n",
    "import random\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data[\"embeddings\"], labels, test_size=0.20)\n",
    "\n",
    "numberOfParents = 40 # number of parents to start\n",
    "numberOfParentsMating = 20 # Number of parents that will mate\n",
    "numberOfParameters = 3  # Number of parameters that will be optimized\n",
    "numberOfGenerations = 100 # Number of genration that will be created \n",
    "\n",
    "# Define the population size\n",
    "populationSize = (numberOfParents, numberOfParameters) # initialize the population with randomly generated parameters\n",
    "\n",
    "population = initilialize_poplulation(numberOfParents) # Define an array to store the fitness  hitory\n",
    "fitnessHistory = np.empty([numberOfGenerations+1, numberOfParents]) # Define an array to store the value of each parameter for each parent and generation\n",
    "populationHistory = np.empty([(numberOfGenerations+1)*numberOfParents, numberOfParameters]) # Insert the value of initial parameters in history\n",
    "\n",
    "populationHistory[0:numberOfParents, :] = population\n",
    "\n",
    "for generation in range(numberOfGenerations):\n",
    "    print(\"This is number %s generation\" % (generation))\n",
    "\n",
    "    xgbDMatrixTrain = xgb.DMatrix(data = X_train, label = y_train)\n",
    "    xgbDMatrixTest  = xgb.DMatrix(data = X_test, label = y_test)\n",
    "    \n",
    "    # Train the dataset and obtain fitness\n",
    "    fitnessValue = train_population(population = population,\n",
    "                                    dMatrixTrain = xgbDMatrixTrain,\n",
    "                                    dMatrixTest =  xgbDMatrixTest,\n",
    "                                    y_test = y_test)\n",
    "\n",
    "    fitnessHistory[generation, :] = fitnessValue\n",
    "    \n",
    "    # Best score in the current iteration\n",
    "    print('Best F1 score in the this iteration = {}'.format(np.max(fitnessHistory[generation, :]))) # Survival of the fittest - take the top parents, based on the fitness value and number of parents needed to be selected\n",
    "    \n",
    "    parents = new_parents_selection(population = population,\n",
    "                                    fitness = fitnessValue,\n",
    "                                    numParents = numberOfParentsMating)\n",
    "    \n",
    "    # Mate these parents to create children having parameters from these parents (we are using uniform crossover)\n",
    "    children = crossover_uniform(parents = parents,\n",
    "                                 childrenSize = (populationSize[0] - parents.shape[0], numberOfParameters))\n",
    "    \n",
    "    # Add mutation to create genetic diversity\n",
    "    children_mutated = mutation(children, numberOfParameters)\n",
    "    \n",
    "    '''\n",
    "    We will create new population, which will contain parents that where selected previously based on the\n",
    "    fitness score and rest of them  will be children\n",
    "    '''\n",
    "    population[0:parents.shape[0], :] = parents # Fittest parents\n",
    "    population[parents.shape[0]:, :] = children_mutated # Children\n",
    "    \n",
    "    populationHistory[(generation+1)*numberOfParents : (generation+1)*numberOfParents+ numberOfParents , :] = population # Srore parent information\n",
    "    \n",
    "#Best solution from the final iteration\n",
    "\n",
    "fitness = train_population(population = population,\n",
    "                           dMatrixTrain = xgbDMatrixTrain,\n",
    "                           dMatrixTest  = xgbDMatrixTest,\n",
    "                           y_test = y_test)\n",
    "\n",
    "fitnessHistory[generation+1, :] = fitness # index of the best solution\n",
    "bestFitnessIndex = np.where(fitness == np.max(fitness))[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "id": "Yio_Aa914EGU",
    "outputId": "4c54e2a9-2596-4394-dc44-9fa808c31ceb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x_fitness = [np.max(fitnessHistory[i]) for i in range(0,fitnessHistory.shape[0])]\n",
    "\n",
    "# best_hyperparams = {}\n",
    "# best_hyperparams['eta'] = population[bestFitnessIndex][0]\n",
    "# best_hyperparams['max_depth'] = int(population[bestFitnessIndex][1])\n",
    "# best_hyperparams['min_child_weight'] = population[bestFitnessIndex][2]\n",
    "\n",
    "# FILE_NAME = 'XGBoost_' + MODEL_TIMESTAMP  + '.jpg'\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(np.arange(len(x_fitness)), x_fitness)\n",
    "# plt.savefig(GA_SCORES_PATH + FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_NAME = 'hyperparams_' + MODEL_TIMESTAMP + '.json'\n",
    "\n",
    "# write_json(best_hyperparams, HYPERPARAMS_PATH, FILE_NAME)\n",
    "# print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBxo17SF46uN"
   },
   "source": [
    "## Train XGBoost with Besthyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vsff9uI41T0",
    "outputId": "c31e178f-c718-40dc-c5e6-9a46e7c48833"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgboost = XGBClassifier(eta = 0.376,\n",
    "                        max_depth = 5,\n",
    "                        min_child_weight = 41,\n",
    "                        tree_method = 'gpu_hist')\n",
    "\n",
    "# xgboost = XGBClassifier(eta = best_hyperparams['eta'],\n",
    "#                         max_depth = best_hyperparams['max_depth'],\n",
    "#                         min_child_weight = best_hyperparams['min_child_weight'],\n",
    "#                         tree_method = 'gpu_hist')\n",
    "\n",
    "xgboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "id": "C7wg6FwZ8ZIG",
    "outputId": "102ffb41-bdd6-4b1c-a306-f70c8a79f533"
   },
   "outputs": [],
   "source": [
    "y_pred = xgboost.predict(X_test)\n",
    "\n",
    "target_names = ['Negativo', 'Positivo']\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=target_names, output_dict = True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "display(report_df)\n",
    "\n",
    "REPORT_NAME  = 'report_' + MODEL_TIMESTAMP + '.csv'\n",
    "\n",
    "report_df.to_csv(REPORTS_PATH + REPORT_NAME, index= True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Main_XGBoost",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
